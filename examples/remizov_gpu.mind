// Copyright 2025-2026 STARGA Inc.
// Licensed under the Apache License, Version 2.0
//
// Remizov ODE Solver — GPU-Parallel Version
// Parallelizes the translation-based solver (Theorem 6) across spatial grid points
//
// Key insight (Remizov, 2025, Remark 9):
//   "Values at different points can be computed in parallel using GPU"
//
// Each grid point x_i computes its solution independently:
//   f(x_i) = sum_k (w_k / lambda) * (S(t_k/n)^n g)(x_i)
//
// The shift operator S(t) reads neighbor values but never writes to them,
// making this embarrassingly parallel across x.
//
// Wall-clock complexity: O(n_iter * n_quad) with n_grid parallel threads

import std.math;
import std.tensor;

// ============================================================================
// Shared utilities (same as remizov_solver.mind)
// ============================================================================

fn linspace(start: f64, end: f64, n: i32) -> tensor<f64[N]> {
    let step = (end - start) / (n - 1) as f64;
    let grid: tensor<f64[N]> = tensor.zeros[f64, (n,)];
    for i in 0..n {
        grid[i] = start + (i as f64) * step;
    }
    return grid;
}

fn interp_linear(x_grid: tensor<f64[N]>, y_grid: tensor<f64[N]>,
                 n: i32, x_query: f64) -> f64 {
    let x_min = x_grid[0];
    let x_max = x_grid[n - 1];
    if x_query <= x_min { return y_grid[0]; }
    if x_query >= x_max { return y_grid[n - 1]; }

    let step = (x_max - x_min) / (n - 1) as f64;
    let idx_f = (x_query - x_min) / step;
    let idx = idx_f as i32;
    if idx >= n - 1 { return y_grid[n - 1]; }

    let t = idx_f - (idx as f64);
    return y_grid[idx] * (1.0 - t) + y_grid[idx + 1] * t;
}

fn gauss_laguerre_nodes_32() -> (tensor<f64[32]>, tensor<f64[32]>) {
    let nodes: tensor<f64[32]> = [
        0.04448936583326,  0.23452610951961,  0.57688462930188,  1.07244875381782,
        1.72240877231768,  2.52833670642579,  3.49221338556548,  4.61645676974976,
        5.90395850417424,  7.35812673318624,  8.98294092421260, 10.78301863254472,
        12.76369798288079, 14.93113975552256, 17.29245433671532, 19.85586237537753,
        22.63089578245498, 25.62863602245921, 28.86210181632637, 32.34662915396487,
        36.10003201710083, 40.14539087991025, 44.51084695665178, 49.23221903498770,
        54.35529462706445, 59.94214052456898, 66.07764919526498, 72.88080035974944,
        80.52493710494128, 89.30314212536448, 99.88923568498693, 113.69397484942860
    ];
    let weights: tensor<f64[32]> = [
        0.11405028393994,  0.26114369910692,  0.39491839613498,  0.50633297605388,
        0.58656244033913,  0.62862456092304,  0.62853611776418,  0.58558127293698,
        0.50271293293702,  0.38668846695345,  0.25755230604698,  0.14484433894272,
        0.06728870721018,  0.02503059878258,  0.00726628914072,  0.00160068088993,
        0.00025655993407,  0.00002869561568,  0.00000210889288,  0.00000009506372,
        0.00000000244637,  0.00000000003202,  0.00000000000019,  0.00000000000000,
        0.00000000000000,  0.00000000000000,  0.00000000000000,  0.00000000000000,
        0.00000000000000,  0.00000000000000,  0.00000000000000,  0.00000000000000
    ];
    return (nodes, weights);
}

// ============================================================================
// GPU-Parallel Shift Operator
// ============================================================================
//
// The key difference from the CPU version: the inner loop over grid points
// runs inside on(gpu0) { parallel for }, distributing each x_i to a GPU thread.

fn apply_shift_operator_gpu(
    x_grid: tensor<f64[N]>,
    h: tensor<f64[N]>,
    n_grid: i32,
    a_vals: tensor<f64[N]>,
    b_vals: tensor<f64[N]>,
    c_vals: tensor<f64[N]>,
    dt: f64
) -> tensor<f64[N]> {
    let h_new: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];

    // Each grid point is independent -> embarrassingly parallel
    on(gpu0) {
        parallel for i in 0..n_grid {
            let xi = x_grid[i];
            let ai = a_vals[i];
            let bi = b_vals[i];
            let ci = c_vals[i];

            let shift_diff = 2.0 * sqrt(ai * dt);
            let shift_drift = 2.0 * bi * dt;

            let h_plus  = interp_linear(x_grid, h, n_grid, xi + shift_diff);
            let h_minus = interp_linear(x_grid, h, n_grid, xi - shift_diff);
            let h_drift = interp_linear(x_grid, h, n_grid, xi + shift_drift);

            h_new[i] = 0.25 * h_plus + 0.25 * h_minus + 0.5 * h_drift + dt * ci * h[i];
        }
    }

    return h_new;
}

// ============================================================================
// GPU-Parallel Core Solver
// ============================================================================

fn remizov_solve_gpu(
    a: fn(f64) -> f64,
    b: fn(f64) -> f64,
    c: fn(f64) -> f64,
    g: fn(f64) -> f64,
    lambda: f64,
    x_min: f64,
    x_max: f64,
    n_grid: i32,
    n_iter: i32,
    n_quad: i32
) -> (tensor<f64[N]>, tensor<f64[N]>) {
    let x_grid = linspace(x_min, x_max, n_grid);

    // Precompute coefficients on grid (can also be GPU-parallel)
    let a_vals: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];
    let b_vals: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];
    let c_vals: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];
    let g_vals: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];

    on(gpu0) {
        parallel for i in 0..n_grid {
            a_vals[i] = a(x_grid[i]);
            b_vals[i] = b(x_grid[i]);
            c_vals[i] = c(x_grid[i]);
            g_vals[i] = g(x_grid[i]);
        }
    }

    let (nodes, weights) = gauss_laguerre_nodes_32();
    let f_solution: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];
    let n_eff = if n_quad > 32 { 32 } else { n_quad };

    // Quadrature loop (sequential — each step depends on the previous)
    for k in 0..n_eff {
        let w_k = weights[k];
        if w_k < 1e-15 { continue; }

        let t_k = nodes[k] / lambda;
        let dt = t_k / (n_iter as f64);

        let h = g_vals;

        // Chernoff iterations with GPU-parallel shift operator
        for iter in 0..n_iter {
            h = apply_shift_operator_gpu(x_grid, h, n_grid, a_vals, b_vals, c_vals, dt);
        }

        // Accumulate (GPU-parallel reduction)
        on(gpu0) {
            parallel for i in 0..n_grid {
                f_solution[i] = f_solution[i] + (w_k / lambda) * h[i];
            }
        }
    }

    return (x_grid, f_solution);
}

// ============================================================================
// Batch Solver: Solve Multiple ODEs Simultaneously
// ============================================================================
//
// When solving the same ODE for multiple lambda values or multiple g functions,
// we can batch across an additional dimension for even more GPU utilization.

fn remizov_solve_batch_lambda(
    a: fn(f64) -> f64,
    b: fn(f64) -> f64,
    c: fn(f64) -> f64,
    g: fn(f64) -> f64,
    lambdas: tensor<f64[M]>,
    n_lambdas: i32,
    x_min: f64,
    x_max: f64,
    n_grid: i32,
    n_iter: i32,
    n_quad: i32
) -> (tensor<f64[N]>, tensor<f64[M, N]>) {
    let x_grid = linspace(x_min, x_max, n_grid);

    let a_vals: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];
    let b_vals: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];
    let c_vals: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];
    let g_vals: tensor<f64[N]> = tensor.zeros[f64, (n_grid,)];

    on(gpu0) {
        parallel for i in 0..n_grid {
            a_vals[i] = a(x_grid[i]);
            b_vals[i] = b(x_grid[i]);
            c_vals[i] = c(x_grid[i]);
            g_vals[i] = g(x_grid[i]);
        }
    }

    // Solution matrix: [n_lambdas, n_grid]
    let f_batch: tensor<f64[M, N]> = tensor.zeros[f64, (n_lambdas, n_grid)];
    let (nodes, weights) = gauss_laguerre_nodes_32();
    let n_eff = if n_quad > 32 { 32 } else { n_quad };

    // Outer loop over lambda values (could be parallelized with streams)
    for m in 0..n_lambdas {
        let lam = lambdas[m];

        for k in 0..n_eff {
            let w_k = weights[k];
            if w_k < 1e-15 { continue; }

            let t_k = nodes[k] / lam;
            let dt = t_k / (n_iter as f64);
            let h = g_vals;

            for iter in 0..n_iter {
                h = apply_shift_operator_gpu(x_grid, h, n_grid, a_vals, b_vals, c_vals, dt);
            }

            on(gpu0) {
                parallel for i in 0..n_grid {
                    f_batch[m, i] = f_batch[m, i] + (w_k / lam) * h[i];
                }
            }
        }
    }

    return (x_grid, f_batch);
}

// ============================================================================
// Benchmark: CPU vs GPU
// ============================================================================

fn coeff_a_const(x: f64) -> f64 { return 1.0; }
fn coeff_b_zero(x: f64) -> f64 { return 0.0; }
fn coeff_c_zero(x: f64) -> f64 { return 0.0; }
fn source_gaussian(x: f64) -> f64 { return exp(-x * x); }

fn main() {
    print("MIND Remizov ODE Solver — GPU-Parallel Version");
    print("===============================================\n");

    let lambda = 4.0;
    let n_grid = 1000;   // Large grid to benefit from GPU parallelism
    let n_iter = 500;
    let n_quad = 24;

    print("Grid size:", n_grid, "points");
    print("Chernoff iterations:", n_iter);
    print("Quadrature nodes:", n_quad);

    // GPU solve
    print("\nSolving on GPU...");
    let (x, f) = remizov_solve_gpu(
        coeff_a_const, coeff_b_zero, coeff_c_zero, source_gaussian,
        lambda, -5.0, 5.0, n_grid, n_iter, n_quad
    );

    // Print solution at selected points
    print("\nSolution at selected points:");
    for i in [0, 250, 500, 750, 999] {
        print("  f(", x[i], ") = ", f[i]);
    }

    // Batch solve: spectral parameter sweep
    print("\n--- Batch Lambda Sweep ---");
    let lambdas: tensor<f64[5]> = [2.0, 4.0, 8.0, 16.0, 32.0];
    let (x_batch, f_batch) = remizov_solve_batch_lambda(
        coeff_a_const, coeff_b_zero, coeff_c_zero, source_gaussian,
        lambdas, 5, -5.0, 5.0, 200, 300, 20
    );

    for m in 0..5 {
        print("lambda =", lambdas[m], " f(0) =", f_batch[m, 100]);
    }

    print("\nDone.");
}
